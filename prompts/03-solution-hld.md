=== RESULT FOR EXECUTION-SPEC ===
Задача: Реализовать backend многопользовательской SaaS-платформы диалогового веб-сайта с AI-агентом (CIP): приём сообщений пользователя, вызов LLM (DeepSeek), потоковая выдача ответа и хранение диалогов с изоляцией по тенанту (tenant_id) и пользователю (user_id).
Выбранный подход: FastAPI-сервер с REST API для отправки сообщений и SSE (Server-Sent Events) для стриминга ответа LLM.
Реализация как SaaS: мультитенантность — все данные и операции изолированы по tenant_id; один инстанс приложения обслуживает множество тенантов; идентификация пользователя (user_id) в рамках тенанта.
Конфигурация приложения и БД — только из .env.
Системный промпт LLM читается из отдельного файла, путь задаётся через конфигурацию (при необходимости — на уровне тенанта).
Диалоги хранятся в PostgreSQL, привязка к tenant_id и user_id.
Клиентский интерфейс чата — страница для встраивания в iframe: форма ввода и приём SSE.
Ключевые инварианты:
Один HTTP-запрос с сообщением порождает ровно один вызов LLM и один SSE-поток.
Все переменные окружения читаются только из .env.
Промпт LLM всегда загружается только из файла.
В БД сообщения всегда привязаны к tenant_id и user_id; диалоги изолированы по тенанту и пользователю.
URL LLM-сервера задаётся только через переменную окружения.
Все API-запросы содержат или подразумевают tenant_id (заголовок, тело, поддомен — единая политика в рамках реализации).
Жёсткие ограничения:
Язык реализации — только Python; стек — FastAPI, PostgreSQL, Docker/docker-compose.
Секреты и конфигурация — только в .env; запрещено хардкодить промпт и URL LLM в коде.
Границы ответственности:
В scope: REST API приёма сообщений, SSE-стрим ответа LLM, персистенс в PostgreSQL, раздача статической страницы для iframe (или документация API для внешнего клиента).
Out of scope: фронтенд вне iframe, админка и инфраструктура за пределами docker-compose, другие протоколы (WebSocket, очереди) без изменения ТЗ.
